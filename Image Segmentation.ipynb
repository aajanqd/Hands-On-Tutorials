{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace = True),  \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unet(nn.Module):\n",
    "    def __init__(self, in_channels = 3, out_channels = 1, channels = [64,128,256,512]):\n",
    "        super(unet, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.ups = nn.ModuleList()\n",
    "        \n",
    "        # building downconv part of unet\n",
    "        for channel in channels:\n",
    "            self.downs.append(DoubleConv(in_channels, channel))\n",
    "            in_channels = channel\n",
    "            \n",
    "        # note that we end channels at 512 and then add a convolution for 1024 channels because of the for loop\n",
    "        # this 1024 is only used once going down, but other values are used going down and up\n",
    "        self.bottleneck = DoubleConv(channels[-1], channels[-1]*2)\n",
    "            \n",
    "        # building upconv part of unet\n",
    "        # note that the length of ups is 2x that of downs bc for every channel you append ConvTranspose2d and DoubleConv\n",
    "        for channel in channels[::-1]:\n",
    "            self.ups.append(nn.ConvTranspose2d(channel*2, channel, kernel_size = 2, stride = 2))\n",
    "            self.ups.append(DoubleConv(channel*2, channel))\n",
    "            \n",
    "        self.final_conv = DoubleConv(channels[0], out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        \n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "            print('downs:',x.shape)\n",
    "            \n",
    "        x = self.bottleneck(x)\n",
    "        print('bottleneck:',x.shape)\n",
    "        \n",
    "        #reverse list in place because channels of skip connections going up is in opposite direction of going down\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        \n",
    "        for i in range(0, len(self.ups), 2):\n",
    "            x = self.ups[i](x) #doing the upconv\n",
    "            sc = skip_connections[i//2]\n",
    "            \n",
    "            if x.shape != sc.shape: #only take :2 because only want height and width, don't watch batch size or channels\n",
    "                x = TF.resize(x, size = sc.shape[:2])\n",
    "            \n",
    "            x = torch.cat((x, sc), dim = 1)\n",
    "            x = self.ups[i+1](x) #doing the downconv\n",
    "            print('ups:',x.shape)\n",
    "            \n",
    "#         print(x.shape, self.final_conv)\n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    x = torch.randn((3,1,160,160))\n",
    "    model = unet(in_channels = 1, out_channels = 1)\n",
    "    preds = model(x)\n",
    "    print(x.shape, preds.shape)\n",
    "    assert x.shape == preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downs: torch.Size([3, 64, 80, 80])\n",
      "downs: torch.Size([3, 128, 40, 40])\n",
      "downs: torch.Size([3, 256, 20, 20])\n",
      "downs: torch.Size([3, 512, 10, 10])\n",
      "bottleneck: torch.Size([3, 1024, 10, 10])\n",
      "ups: torch.Size([3, 512, 20, 20])\n",
      "ups: torch.Size([3, 256, 40, 40])\n",
      "ups: torch.Size([3, 128, 80, 80])\n",
      "ups: torch.Size([3, 64, 160, 160])\n",
      "torch.Size([3, 1, 160, 160]) torch.Size([3, 1, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
